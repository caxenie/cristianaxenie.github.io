---
layout: archive
permalink: /hackatons/
title: "Hackatons"
author_profile: true
---

<details>
	<summary><h3>Merck Research "Future of AI Challenge" (August 2019)</h3></symmary>
IRENA (Invariant Representations Extraction in Neural Architectures): 1st Place at Merck Future of AI Research Challenge.

Media coverage:	
<a href="https://app.ekipa.de/challenge/future-of-ai/about">https://app.ekipa.de/challenge/future-of-ai/about</a>
<a href="https://www.thi.de/suche/news/news/thi-erfolgreich-in-ai-forschungswettbewerb">https://www.thi.de/suche/news/news/thi-erfolgreich-in-ai-forschungswettbewerb</a>
<p align="justify">Merck Research Challenge aimed to generate insights from various disciplines that can lead to progress towards an understanding of invariant representation - that are novel and not based on Deep Learning.</p>
<p align="justify">IRENA (Invariant Representations Extraction in Neural Architectures) is the approach that team NeuroTHIx developed. IRENA offers a computational layer for extracting sensory relations for rich visual scenes, withy learning, inference, de-noising and sensor fusion capabilities. The system is also capable, through its underlying unsupervised learning capabilities, to embed semantics and perform scene understanding.</p>
Using cortical maps as neural substrate for distributed representations of sensory streams, our system is able to learn its connectivity (i.e., structure) from the long-term evolution of sensory observations. This process mimics a typical development process where self-construction (connectivity learning), self-organization, and correlation extraction ensure a refined and stable representation and processing substrate. Following these principles, we propose a model based on Self-Organizing Maps (SOM) and Hebbian Learning (HL) as main ingredients for extracting underlying correlations in sensory data, the basis for subsequently extracting invariant representations.

&nbsp;

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure6bw.png"><img class="wp-image-26722 alignleft" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure6bw.png" alt="" width="343" height="334" /></a>

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure7bw.png">
	
<img class="wp-image-26725 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure7bw-1024x638.png" alt="" width="503" height="314" /></a>

&nbsp;

<hr />

</details>

<h3>University of Cambridge Hackathon - Hack Cambridge (January 2017)</h3>
&nbsp;

Microsoft Faculty Connection coverage (<a href="http://goo.gl/uPWGna">http://goo.gl/uPWGna</a>) for project demo at Hack Cambridge 2017, 28 – 29 January 2017, University of Cambridge with a Real-time Event-based Vision Monitoring and Notification System for Seniors and Elderly using Neural Networks.

It has been estimated that 33% of people age 65 will fall. At around 80, that increases to 50%. In case of a fall, seniors who receive help within an hour have a better rate of survival and, the faster help arrives, the less likely an injury will lead to hospitalization or the need to move into a long-term care facility. In such cases fast visual detection of abnormal motion patterns is crucial.

In this project we propose the use of a novel embedded Dynamic Vision Sensor (eDVS) for the task of classifying falls. Opposite from standard cameras which provide a time sequenced stream of frames, the eDVS provides only relative changes in a scene, given by individual events at the pixel level. Using this different encoding scheme the eDVS brings advantages over standard cameras. First, there is no redundancy in the data received from the sensor, only changes are reported. Second, as only events are considered the eDVS data rate is high. Third, the power consumption of the overall system is small, as just a low-end microcontroller is used to fetch events from the sensor and can ultimately run for long time periods in a battery powered setup. This project investigates how can we exploit the eDVS fast response time and low-redundancy in making decisions about elderly motion.

The computation back-end will be realized with a neural network classification to detect fall and filter outliers. The data will be provided from 2 stimuli (blinking LEDs at different frequencies) and will represent the actual position of the person wearing them. The changes in position of the stimuli will encode the possible positions corresponding to falls or normal cases.

We will use Microsoft Azure ML Studio to implement a MLP binary classifier for the 4 (2 stimuli x 2 Cartesian coordinates - (x,y) in the field of view) dimensional input. We labelled the data with Fall (F) and No Fall (NF).

&nbsp;

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/cambridge_hack.png"><img class="wp-image-24230 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/cambridge_hack.png" alt="" width="809" height="518" /></a>

<hr />

<h3>ANDRITZ Pioneers Hackathon (January 2017)</h3>
&nbsp;

Best innovation idea at the ANDRITZ Pioneers Hackaton innovating for the international technology group ANDRITZ. Developed an artificial neural learning agent for automation process productivity enhancement.

&nbsp;

<img class="wp-image-24227 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/andritz_hack.png" alt="" width="760" height="412" />

<hr />

<h3>Wellcome Trust Hack The Senses Hackathon (June 2016)</h3>

&nbsp;

WIRED UK coverage ( <a href="http://goo.gl/5yQ1Fn">http://goo.gl/5yQ1Fn</a> ) at the Hack the Senses hackathon in London: How to hack your senses: from 'seeing' sound to 'hair GPS': "Two-man team HearSee built a headband that taps into synaesthesia, translating changes in frame-less video to sound allowing blind people or those with a weak vision to see motion. Roboticist and neurologist Cristian Axenie assembled the hardware in mere minutes – attaching a pair of cameras and wires to a terrycloth headband."

&nbsp;

<hr />

<h3>Daimler FinTech Hackathon (April 2016)</h3>
&nbsp;

Awarded 1st prize (team) at the Daimler Financial Services Big Data Analytics Hackaton for the design of a neuro-fuzzy learning system for anomaly detection and user interaction in big data streams.

&nbsp;

<img class="wp-image-24215 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/daimler_hack.png" alt="" width="797" height="487" />

<hr />

<h3>Burda Hackdays (April 2016)</h3>
&nbsp;

Awarded special Microsoft Cognitive Technologies prize at the Burda Hackdays for the design of a neural learning system for inferring role assignments in working teams using psychometric data analytics.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/burda_hack.png"><img class="wp-image-24212 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/burda_hack.png" alt="" width="781" height="446" /></a>

<hr />

<h3>Automotive Hackdays (March 2016)</h3>

&nbsp;

Awarded 1st prize (team) in the BMW Automotive Hackdays for the design of an inference system for driving profile learning and recommendation for skills improvement and predictive maintenance in car-sharing.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/bmw_hack.png"><img class="wp-image-24209 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/bmw_hack.png" alt="" width="774" height="395" /></a>

</section>
